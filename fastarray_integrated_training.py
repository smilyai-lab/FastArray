"""
SAM1 Training with FastArray Integration - Complete Version
This version integrates FastArray compression for model parameters while maintaining JAX/TPU compatibility
"""

# ==============================================================================
# IMPORTS AND INITIAL SETUP
# ==============================================================================

import os
import time
import json
from functools import partial
from typing import Any, Optional
from datetime import timedelta

print("="*70)
print("üîß INSTALLING REQUIRED PACKAGES & IMPORTING FASTARRAY".center(70))
print("="*70)

try:
    import safetensors
except ImportError:
    print("Installing safetensors...")
    os.system("pip install -q safetensors")

# Import FastArray first to set up backend
import fastarray as fa  
print(f"‚úÖ FastArray imported - Compression ready!")
print(f"   Available backends: {fa.backend.get_available_backends()}")

# Set backend for TPU operations
fa.backend.set_backend('tpu')

import jax
import jax.numpy as jnp
from jax import random
from jax import tree_util
from jax.sharding import PartitionSpec as P, NamedSharding, Mesh
from jax.experimental import mesh_utils
import flax.linen as nn
from flax.training import train_state, checkpoints
import optax
import numpy as np
import pandas as pd
from tokenizers import Tokenizer
from safetensors.flax import save_file

# Import FastArray JAX integration
from fastarray.jax_training_integration import integrate_fastarray_in_training, get_compression_stats

print(f"\nüöÄ JAX TPU INITIALIZATION with FastArray Integration")
devices = jax.devices()
print(f"‚úÖ Found {len(devices)} devices: {devices[0].platform}")

mesh_shape = (1, 8)  # Example TPU mesh
try:
    mesh_devices = mesh_utils.create_device_mesh(mesh_shape)
    mesh = Mesh(mesh_devices, axis_names=('data', 'model'))
    print(f"üîÄ TPU Mesh: {mesh_shape}")
except:
    # Fallback if TPU mesh creation fails
    mesh = jax.sharding.Mesh(np.array(jax.devices()[:1]), axis_names=('data', 'model'))
    print(f"üîÄ Fallback single-device mesh created")

# ==============================================================================
# ‚öôÔ∏è CHAT TEMPLATE CONFIGURATION
# ==============================================================================

CHAT_USER_PREFIX = "User:"
CHAT_ASSISTANT_PREFIX = "Sam:"
CHAT_TEMPLATE = f"{CHAT_USER_PREFIX} {{{{input}}}}\n{CHAT_ASSISTANT_PREFIX} {{{{output}}}}"

print(f"\nüí¨ Chat Template:")
print(f"   User: {CHAT_USER_PREFIX}")
print(f"   Assistant: {CHAT_ASSISTANT_PREFIX}")

# ==============================================================================
# CONFIGURATION
# ==============================================================================

class Config:
    vocab_size: int = 50257
    d_model: int = 1024
    n_layers: int = 24
    n_heads: int = 16
    n_kv_heads: int = 2
    ff_mult: float = 2.5
    max_len: int = 1024
    dropout: float = 0.1
    layer_drop_prob: float = 0.05

    rope_theta: float = 10_000.0
    yarn_scale: float = 1.0
    yarn_alpha: float = 1.0
    yarn_beta: float = 32.0
    use_yarn: bool = True
    use_alibi: bool = True
    alibi_weight: float = 0.3

    use_z_loss: bool = True
    z_loss_weight: float = 1e-4
    label_smoothing: float = 0.05
    use_remat: bool = True

    dtype: Any = jnp.bfloat16
    param_dtype: Any = jnp.bfloat16

    optimizer: str = "lion"
    lr: float = 1e-4
    weight_decay: float = 0.1
    beta1: float = 0.9
    beta2: float = 0.99

    warmup_steps: int = 500
    max_steps: int = 10_000
    epochs: int = 3
    schedule_type: str = "onecycle"

    per_device_batch: int = 4
    grad_accum_steps: int = 2
    global_batch: int = per_device_batch * len(devices) * grad_accum_steps

    data_parallel: int = mesh_shape[0]
    model_parallel: int = mesh_shape[1]

    ff_dim: int = int(d_model * ff_mult)
    head_dim: int = d_model // n_heads
    kv_head_dim: int = d_model // n_kv_heads
    seed: int = 42

cfg = Config()

cfg.ff_dim = ((cfg.ff_dim + cfg.model_parallel - 1) // cfg.model_parallel) * cfg.model_parallel
if cfg.vocab_size % cfg.model_parallel != 0:
    original_vocab = cfg.vocab_size
    cfg.vocab_size = ((cfg.vocab_size + cfg.model_parallel - 1) // cfg.model_parallel) * cfg.model_parallel
    print(f"‚ö†Ô∏è  Vocab adjusted: {original_vocab} ‚Üí {cfg.vocab_size}")

print(f"\n‚öôÔ∏è Model: {cfg.d_model}d √ó {cfg.n_layers}L √ó {cfg.n_heads}H")
print(f"   Optimizer: Lion (LR={cfg.lr})")
print(f"   Batch: {cfg.global_batch}")

OUTPUT_DIR = "/kaggle/working/sam1-600m-lion-fast/"
CHECKPOINT_DIR = os.path.join(OUTPUT_DIR, "checkpoints")
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(CHECKPOINT_DIR, exist_ok=True)

# ==============================================================================
# MODEL AND SUPPORTING FUNCTIONS (same as original)
# ==============================================================================

def yarn_find_correction_dim(num_rotations, dim, base=10000, max_position_embeddings=2048):
    return (dim * jnp.log(max_position_embeddings / (num_rotations * 2 * jnp.pi))) / (2 * jnp.log(base))

def yarn_find_correction_range(low_rot, high_rot, dim, base=10000, max_position_embeddings=2048):
    low = jnp.floor(yarn_find_correction_dim(low_rot, dim, base, max_position_embeddings))
    high = jnp.ceil(yarn_find_correction_dim(high_rot, dim, base, max_position_embeddings))
    return jnp.maximum(low, 0).astype(jnp.int32), jnp.minimum(high, dim - 1).astype(jnp.int32)

def yarn_linear_ramp_mask(min_val, max_val, dim):
    if min_val == max_val:
        max_val += 0.001
    linear_func = (jnp.arange(dim, dtype=jnp.float32) - min_val) / (max_val - min_val)
    return jnp.clip(linear_func, 0, 1)

def yarn_get_mscale(scale=1.0, mscale=1.0):
    if scale <= 1:
        return 1.0
    return 0.1 * mscale * jnp.log(scale) + 1.0

def precompute_yarn_freqs(dim: int, end: int, theta: float = 10000.0,
                          scale: float = 1.0, alpha: float = 1.0,
                          beta: float = 32.0, dtype=jnp.bfloat16):
    freqs = 1.0 / (theta ** (jnp.arange(0, dim, 2, dtype=jnp.float32) / dim))

    if scale > 1.0:
        low, high = yarn_find_correction_range(beta, alpha, dim, theta, int(end * scale))
        inv_freq_mask = 1.0 - yarn_linear_ramp_mask(low, high, dim // 2)
        freqs = freqs / ((1 - inv_freq_mask) * (scale - 1) + 1)

    t = jnp.arange(end, dtype=jnp.float32)
    freqs = jnp.outer(t, freqs)
    mscale = yarn_get_mscale(scale)

    cos = jnp.cos(freqs) * mscale
    sin = jnp.sin(freqs) * mscale

    return jnp.concatenate([cos, sin], axis=-1).astype(dtype), mscale

def get_alibi_slopes(n_heads: int):
    def get_slopes_power_of_2(n):
        start = 2 ** (-(2 ** -(np.log2(n) - 3)))
        ratio = start
        return [start * ratio ** i for i in range(n)]

    if np.log2(n_heads).is_integer():
        return jnp.array(get_slopes_power_of_2(n_heads))
    else:
        closest_power_of_2 = 2 ** np.floor(np.log2(n_heads))
        slopes = get_slopes_power_of_2(int(closest_power_of_2))
        slopes_extra = get_slopes_power_of_2(2 * int(closest_power_of_2))
        slopes_extra = slopes_extra[0::2][:int(n_heads - closest_power_of_2)]
        return jnp.array(slopes + slopes_extra)

def create_alibi_bias(seq_len: int, n_heads: int):
    positions = jnp.arange(seq_len)
    position_diff = positions[None, :] - positions[:, None]
    slopes = get_alibi_slopes(n_heads)
    alibi = slopes[:, None, None] * position_diff[None, :, :]
    return alibi[None, :, :, :].astype(jnp.bfloat16)

def apply_rotary_emb(xq, xk, freqs_cis, mscale=1.0):
    def rotate_half(x):
        x1, x2 = jnp.split(x, 2, axis=-1)
        return jnp.concatenate([-x2, x1], axis=-1)

    seq_len = xq.shape[2]
    head_dim = xq.shape[3]

    freqs = freqs_cis[:seq_len, :]
    half_dim = head_dim // 2
    cos = freqs[:, :half_dim]
    sin = freqs[:, half_dim:]

    cos = jnp.repeat(cos, 2, axis=-1)
    sin = jnp.repeat(sin, 2, axis=-1)

    cos = cos[None, None, :, :]
    sin = sin[None, None, :, :]

    xq_out = (xq * cos) + (rotate_half(xq) * sin)
    xk_out = (xk * cos) + (rotate_half(xk) * sin)

    return xq_out, xk_out

# Model components (same as original)
class RMSNorm(nn.Module):
    epsilon: float = 1e-5
    dtype: Any = jnp.bfloat16

    @nn.compact
    def __call__(self, x):
        x = x.astype(jnp.float32)
        scale = self.param('scale', nn.initializers.ones, (x.shape[-1],))
        variance = jnp.mean(jnp.square(x), axis=-1, keepdims=True)
        x = x * jax.lax.rsqrt(variance + self.epsilon) * scale
        return x.astype(self.dtype)

class GroupedQueryAttention(nn.Module):
    d_model: int
    n_heads: int
    n_kv_heads: int
    dropout: float
    freqs_cis: jnp.ndarray
    yarn_mscale: float
    alibi_bias: Optional[jnp.ndarray]
    alibi_weight: float
    dtype: Any = jnp.bfloat16

    @nn.compact
    def __call__(self, x, mask, training: bool = False):
        B, T, D = x.shape
        head_dim = self.d_model // self.n_heads
        n_rep = self.n_heads // self.n_kv_heads

        q = nn.Dense(self.d_model, use_bias=False,
                     kernel_init=nn.initializers.normal(stddev=0.02),
                     dtype=self.dtype, name='q_proj')(x)

        kv_dim = self.d_model * self.n_kv_heads // self.n_heads
        k = nn.Dense(kv_dim, use_bias=False,
                     kernel_init=nn.initializers.normal(stddev=0.02),
                     dtype=self.dtype, name='k_proj')(x)
        v = nn.Dense(kv_dim, use_bias=False,
                     kernel_init=nn.initializers.normal(stddev=0.02),
                     dtype=self.dtype, name='v_proj')(x)

        q = q.reshape(B, T, self.n_heads, head_dim).transpose(0, 2, 1, 3)
        k = k.reshape(B, T, self.n_kv_heads, head_dim).transpose(0, 2, 1, 3)
        v = v.reshape(B, T, self.n_kv_heads, head_dim).transpose(0, 2, 1, 3)

        k = jnp.repeat(k, n_rep, axis=1)
        v = jnp.repeat(v, n_rep, axis=1)

        q, k = apply_rotary_emb(q, k, self.freqs_cis, self.yarn_mscale)

        scores = jnp.matmul(q, k.transpose(0, 1, 3, 2)) / jnp.sqrt(head_dim)

        if self.alibi_bias is not None:
            scores = scores * (1 - self.alibi_weight)
            alibi = self.alibi_bias[:, :, :T, :T]
            scores = scores + (alibi * self.alibi_weight)

        scores = scores + mask

        attn_weights = nn.softmax(scores.astype(jnp.float32), axis=-1).astype(self.dtype)
        attn_weights = nn.Dropout(self.dropout, deterministic=not training)(attn_weights)

        attn_out = jnp.matmul(attn_weights, v)
        attn_out = attn_out.transpose(0, 2, 1, 3).reshape(B, T, D)

        out = nn.Dense(self.d_model, use_bias=False,
                      kernel_init=nn.initializers.normal(stddev=0.02),
                      dtype=self.dtype, name='o_proj')(attn_out)

        return nn.Dropout(self.dropout, deterministic=not training)(out)

class SwiGLU(nn.Module):
    d_model: int
    ff_dim: int
    dropout: float
    dtype: Any = jnp.bfloat16

    @nn.compact
    def __call__(self, x, training: bool = False):
        gate = nn.Dense(self.ff_dim, use_bias=False,
                       kernel_init=nn.initializers.normal(stddev=0.02),
                       dtype=self.dtype, name='gate_proj')(x)
        up = nn.Dense(self.ff_dim, use_bias=False,
                     kernel_init=nn.initializers.normal(stddev=0.02),
                     dtype=self.dtype, name='up_proj')(x)
        hidden = nn.silu(gate) * up
        out = nn.Dense(self.d_model, use_bias=False,
                      kernel_init=nn.initializers.normal(stddev=0.02),
                      dtype=self.dtype, name='down_proj')(hidden)
        return nn.Dropout(self.dropout, deterministic=not training)(out)

class TransformerBlock(nn.Module):
    d_model: int
    n_heads: int
    n_kv_heads: int
    ff_dim: int
    dropout: float
    freqs_cis: jnp.ndarray
    yarn_mscale: float
    alibi_bias: Optional[jnp.ndarray]
    alibi_weight: float
    layer_idx: int
    layer_drop_prob: float = 0.0
    dtype: Any = jnp.bfloat16

    @nn.compact
    def __call__(self, x, mask, training: bool = False):
        if training and self.layer_drop_prob > 0:
            survival_prob = 1.0 - self.layer_drop_prob
            rng = self.make_rng('dropout')
            keep = jax.random.bernoulli(rng, survival_prob)
            scale = jnp.where(keep, 1.0 / survival_prob, 0.0)
        else:
            scale = 1.0

        h = RMSNorm(dtype=self.dtype, name='attn_norm')(x)
        h = GroupedQueryAttention(
            self.d_model, self.n_heads, self.n_kv_heads, self.dropout,
            self.freqs_cis, self.yarn_mscale, self.alibi_bias,
            self.alibi_weight, dtype=self.dtype, name='attn'
        )(h, mask, training)
        x = x + h * scale

        h = RMSNorm(dtype=self.dtype, name='ffn_norm')(x)
        h = SwiGLU(self.d_model, self.ff_dim, self.dropout,
                   dtype=self.dtype, name='ffn')(h, training)
        x = x + h * scale

        return x

class SAM1Model(nn.Module):
    config: Config

    @nn.compact
    def __call__(self, input_ids, training: bool = False):
        cfg = self.config

        freqs_cis, yarn_mscale = precompute_yarn_freqs(
            cfg.head_dim, cfg.max_len, cfg.rope_theta,
            cfg.yarn_scale, cfg.yarn_alpha, cfg.yarn_beta, cfg.dtype
        )

        alibi_bias = None
        if cfg.use_alibi:
            alibi_bias = create_alibi_bias(cfg.max_len, cfg.n_heads)

        x = nn.Embed(cfg.vocab_size, cfg.d_model,
                    embedding_init=nn.initializers.normal(stddev=0.02),
                    dtype=cfg.dtype, name='embed_tokens')(input_ids)

        seq_len = input_ids.shape[1]
        mask = jnp.tril(jnp.ones((seq_len, seq_len)))
        mask = jnp.where(mask == 0, -1e9, 0.0).astype(cfg.dtype)

        if cfg.use_remat:
            RematTransformerBlock = nn.remat(
                TransformerBlock,
                policy=jax.checkpoint_policies.nothing_saveable,
                prevent_cse=False,
                static_argnums=(3,)
            )
        else:
            RematTransformerBlock = TransformerBlock

        for i in range(cfg.n_layers):
            x = RematTransformerBlock(
                cfg.d_model, cfg.n_heads, cfg.n_kv_heads, cfg.ff_dim,
                cfg.dropout, freqs_cis, yarn_mscale, alibi_bias,
                cfg.alibi_weight, layer_idx=i,
                layer_drop_prob=cfg.layer_drop_prob,
                dtype=cfg.dtype, name=f'layers_{i}'
            )(x, mask, training)

        x = RMSNorm(dtype=cfg.dtype, name='norm')(x)

        logits = nn.Dense(cfg.vocab_size, use_bias=False,
                         kernel_init=nn.initializers.normal(stddev=0.02),
                         dtype=cfg.dtype, name='lm_head')(x)

        return logits

# Optimizer functions (same as original)
def create_optimized_schedule(config: Config):
    if config.schedule_type == "onecycle":
        peak_lr = config.lr
        warmup_lr = peak_lr / 5.0
        final_lr = peak_lr / 10.0

        warmup_fn = optax.linear_schedule(
            warmup_lr, peak_lr, config.warmup_steps
        )

        main_steps = config.max_steps - config.warmup_steps
        main_fn = optax.cosine_decay_schedule(
            peak_lr, main_steps, alpha=final_lr/peak_lr
        )

        schedule = optax.join_schedules(
            [warmup_fn, main_fn], [config.warmup_steps]
        )

    elif config.schedule_type == "wsd":
        stable_steps = int(config.max_steps * 0.9)
        decay_steps = config.max_steps - stable_steps - config.warmup_steps

        warmup_fn = optax.linear_schedule(0.0, config.lr, config.warmup_steps)
        stable_fn = optax.constant_schedule(config.lr)
        decay_fn = optax.cosine_decay_schedule(config.lr, decay_steps, 0.1)

        schedule = optax.join_schedules(
            [warmup_fn, stable_fn, decay_fn],
            [config.warmup_steps, config.warmup_steps + stable_steps]
        )

    else:
        warmup_fn = optax.linear_schedule(0.0, config.lr, config.warmup_steps)
        decay_fn = optax.cosine_decay_schedule(
            config.lr, config.max_steps - config.warmup_steps, 0.1
        )
        schedule = optax.join_schedules(
            [warmup_fn, decay_fn], [config.warmup_steps]
        )

    return schedule

# Sharding functions (same as original)
def create_sharding_rules(mesh: Mesh, config: Config):
    d_shardable = config.d_model % config.model_parallel == 0
    ff_shardable = config.ff_dim % config.model_parallel == 0

    rules = {
        'embed_tokens/embedding': P(None, 'model') if d_shardable else P(None, None),
        'q_proj/kernel': P(None, 'model') if d_shardable else P(None, None),
        'k_proj/kernel': P(None, 'model') if d_shardable else P(None, None),
        'v_proj/kernel': P(None, 'model') if d_shardable else P(None, None),
        'o_proj/kernel': P('model', None) if d_shardable else P(None, None),
        'gate_proj/kernel': P(None, 'model') if ff_shardable else P(None, None),
        'up_proj/kernel': P(None, 'model') if ff_shardable else P(None, None),
        'down_proj/kernel': P('model', None) if ff_shardable else P(None, None),
        'attn_norm/scale': P(None,),
        'ffn_norm/scale': P(None,),
        'norm/scale': P(None,),
        'lm_head/kernel': P('model', None) if d_shardable else P(None, None),
    }

    return rules, {'input': P('data', None, None), 'logits': P('data', None, 'model')}

# Training state with FastArray integration
class TrainState(train_state.TrainState):
    dropout_rng: jax.random.PRNGKey
    compressed_params: dict = None  # Store compressed version for efficient saving

def create_train_state_with_fastarray_compression(rng, config: Config, mesh: Mesh, sharding_rules: dict):
    """Create train state with FastArray compression for parameter storage"""
    model = SAM1Model(config=config)
    dummy_input = jnp.ones((1, config.max_len), dtype=jnp.int32)
    variables = model.init(rng, dummy_input, training=False)
    original_params = variables['params']

    param_count = sum(x.size for x in tree_util.tree_leaves(original_params))
    print(f"\nüìä Model: {param_count:,} params (~{param_count/1e6:.1f}M)")
    
    # COMPRESS PARAMETERS WITH FASTARRAY FOR STORAGE
    print(f"üîÑ Compressing model parameters with FastArray...")
    start_time = time.time()
    
    def compress_param(param):
        if isinstance(param, jnp.ndarray):
            # Convert JAX array to numpy, then to FastArray for compression
            np_param = np.array(param)
            fa_param = fa.array(np_param, compression="quantization")
            return fa_param
        else:
            return param
    
    compressed_params = tree_util.tree_map(compress_param, original_params)
    
    # Calculate compression stats
    stats = get_compression_stats(compressed_params)
    print(f"‚úÖ Compression completed in {time.time() - start_time:.2f}s")
    print(f"   Original: {stats['original_size_mb']:.2f}MB")
    print(f"   Compressed: {stats['compressed_size_mb']:.2f}MB")
    print(f"   Saved: {stats['saved_mb']:.2f}MB ({stats['compression_ratio']:.1f}x)")
    
    # Convert compressed params back to sharded JAX arrays for computation
    print(f"üîÑ Converting compressed parameters to JAX for TPU computation...")
    
    def get_param_spec(path, sharding_rules):
        path_str = '/'.join(str(p.key) for p in path)
        for pattern, spec in sharding_rules.items():
            if pattern in path_str:
                return spec
        return P()

    def convert_compressed_to_jax(path, fa_param):
        if isinstance(fa_param, fa.FastArray):
            # Convert FastArray to JAX array using the integration function
            jax_param = fa.jax_integration.to_jax_array(fa_param)
            # Apply sharding
            spec = get_param_spec(path, sharding_rules)
            sharding = NamedSharding(mesh, spec)
            return jax.device_put(jax_param, sharding)
        else:
            # If already a JAX array, return as is
            return jnp.array(fa_param)
    
    jax_params = tree_util.tree_map_with_path(convert_compressed_to_jax, compressed_params)

    schedule_fn = create_optimized_schedule(config)

    if config.optimizer == "lion":
        print(f"ü¶Å Lion optimizer (LR={config.lr})")
        optimizer = optax.chain(
            optax.clip_by_global_norm(1.0),
            optax.scale_by_lion(b1=config.beta1, b2=config.beta2),
            optax.add_decayed_weights(config.weight_decay),
            optax.scale_by_schedule(schedule_fn),
            optax.scale(-1.0)
        )
    else:
        optimizer = optax.chain(
            optax.clip_by_global_norm(1.0),
            optax.adamw(schedule_fn, weight_decay=config.weight_decay,
                       b1=config.beta1, b2=config.beta2)
        )

    state = TrainState.create(
        apply_fn=model.apply,
        params=jax_params,
        tx=optimizer,
        dropout_rng=rng,
        compressed_params=compressed_params  # Store compressed version for saving
    )
    return state, param_count

# Loss function (same as original)
def compute_loss(logits, targets, use_z_loss=True, z_loss_weight=1e-4,
                label_smoothing=0.0):
    logits_fp32 = logits.astype(jnp.float32)
    vocab_size = logits_fp32.shape[-1]
    logits_flat = logits_fp32.reshape(-1, vocab_size)
    targets_flat = targets.reshape(-1)

    if label_smoothing > 0:
        targets_onehot = jax.nn.one_hot(targets_flat, vocab_size)
        smooth_targets = (1 - label_smoothing) * targets_onehot + \
                        label_smoothing / vocab_size
        log_probs = jax.nn.log_softmax(logits_flat, axis=-1)
        ce_loss = -jnp.sum(smooth_targets * log_probs, axis=-1)
        ce_loss = jnp.mean(ce_loss)
    else:
        ce_loss = optax.softmax_cross_entropy_with_integer_labels(
            logits_flat, targets_flat
        )
        ce_loss = jnp.mean(ce_loss)

    z_loss = 0.0
    if use_z_loss:
        z_loss = jnp.square(jax.nn.logsumexp(logits_flat, axis=-1))
        z_loss = jnp.mean(z_loss) * z_loss_weight

    total_loss = ce_loss + z_loss

    predictions = jnp.argmax(logits_flat, axis=-1)
    accuracy = jnp.mean((predictions == targets_flat).astype(jnp.float32))

    return total_loss, {
        'ce_loss': ce_loss,
        'z_loss': z_loss,
        'accuracy': accuracy,
        'perplexity': jnp.exp(ce_loss)
    }

# Training and eval steps (same as original)
@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(2, 3, 4))
def train_step(state, batch, use_z_loss, z_loss_weight, label_smoothing):
    def loss_fn(params):
        input_ids = batch[:, :-1]
        targets = batch[:, 1:]
        logits = state.apply_fn(
            {'params': params},
            input_ids,
            training=True,
            rngs={'dropout': state.dropout_rng}
        )
        loss, metrics = compute_loss(
            logits, targets, use_z_loss, z_loss_weight, label_smoothing
        )
        return loss, metrics

    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)
    (loss, metrics), grads = grad_fn(state.params)

    grads = jax.lax.pmean(grads, axis_name='batch')
    loss = jax.lax.pmean(loss, axis_name='batch')
    metrics = jax.lax.pmean(metrics, axis_name='batch')

    state = state.apply_gradients(grads=grads)

    new_dropout_rng = jax.random.fold_in(state.dropout_rng, state.step)
    state = state.replace(dropout_rng=new_dropout_rng)

    return state, metrics

@partial(jax.pmap, axis_name='batch', static_broadcasted_argnums=(2, 3, 4))
def eval_step(state, batch, use_z_loss, z_loss_weight, label_smoothing):
    input_ids = batch[:, :-1]
    targets = batch[:, 1:]
    logits = state.apply_fn({'params': state.params}, input_ids, training=False)
    loss, metrics = compute_loss(
        logits, targets, use_z_loss, z_loss_weight, label_smoothing
    )
    loss = jax.lax.pmean(loss, axis_name='batch')
    metrics = jax.lax.pmean(metrics, axis_name='batch')
    return metrics

# Test generation function (same as original)
def test_generation(state, tokenizer, config, step, prompt=None):
    """Generate text to verify model is learning - catches gibberish EARLY!"""
    if prompt is None:
        prompt = f"{CHAT_USER_PREFIX} Hello, how are you?\n{CHAT_ASSISTANT_PREFIX}"

    print(f"\n{'='*70}")
    print(f"üß™ TEST GENERATION (Step {step})".center(70))
    print(f"{'='*70}")

    encoding = tokenizer.encode(prompt)
    input_ids = jnp.array(encoding.ids)[None, :]

    if input_ids.shape[1] > config.max_len:
        input_ids = input_ids[:, -config.max_len:]

    print(f"Prompt: {repr(prompt[:50])}")

    # For generation, we can get a single parameter copy to generate
    params = tree_util.tree_map(lambda x: x[0], state.params)

    rng = random.PRNGKey(42)
    generated_ids = input_ids
    max_new_tokens = 50
    temperature = 0.8

    for i in range(max_new_tokens):
        logits = state.apply_fn({'params': params}, generated_ids, training=False)

        next_logits = logits[0, -1, :] / temperature
        top_k = 50
        top_k_logits, top_k_indices = jax.lax.top_k(next_logits, top_k)
        next_logits_filtered = jnp.full_like(next_logits, -1e9)
        next_logits_filtered = next_logits_filtered.at[top_k_indices].set(top_k_logits)

        rng, sample_rng = random.split(rng)
        next_token = random.categorical(sample_rng, next_logits_filtered)[None, None]

        generated_ids = jnp.concatenate([generated_ids, next_token], axis=1)

        if next_token[0, 0] == tokenizer.token_to_id(""):
            break

    generated_text = tokenizer.decode(generated_ids[0].tolist())

    if CHAT_ASSISTANT_PREFIX in generated_text:
        response = generated_text.split(CHAT_ASSISTANT_PREFIX)[-1].strip()
    else:
        response = generated_text

    print(f"\n{'‚îÄ'*70}")
    print(f"Generated Response:")
    print(f"{'‚îÄ'*70}")
    print(response[:200])
    print(f"{'‚îÄ'*70}")

    checks = []
    checks.append(("Length", len(response), "‚úÖ" if len(response) > 10 else "‚ùå"))
    checks.append(("Not repetitive", len(set(response.split()[:20])),
                   "‚úÖ" if len(set(response.split()[:20])) > 5 else "‚ö†Ô∏è"))
    checks.append(("Has words", any(c.isalpha() for c in response),
                   "‚úÖ" if any(c.isalpha() for c in response) else "‚ùå"))

    print("\nüìä Quality Checks:")
    for name, value, status in checks:
        print(f"   {status} {name}: {value}")

    if any(status == "‚ùå" for _, _, status in checks):
        print("\n‚ö†Ô∏è  WARNING: Model output quality is poor!")
        print("   Consider stopping training if this persists.")
        return False

    print(f"{'='*70}\n")
    return True

# ETA tracker (same as original)
class ETATracker:
    def __init__(self, total_steps):
        self.total_steps = total_steps
        self.start_time = time.time()
        self.step_times = []
        self.current_step = 0

    def update(self, step, step_time):
        self.current_step = step
        self.step_times.append(step_time)
        if len(self.step_times) > 50:
            self.step_times.pop(0)

    def get_eta(self):
        if len(self.step_times) < 5:
            return "Calculating..."
        avg_step_time = np.mean(self.step_times[-20:])
        steps_remaining = self.total_steps - self.current_step
        seconds_remaining = avg_step_time * steps_remaining
        return str(timedelta(seconds=int(seconds_remaining)))

    def get_elapsed(self):
        elapsed = time.time() - self.start_time
        return str(timedelta(seconds=int(elapsed)))

# ==============================================================================
# MAIN TRAINING LOOP
# ==============================================================================

print("\nüìä Loading dataset...")

# For this demo, we'll use dummy data if files don't exist
CACHE_DIR = "/kaggle/input/ultra-dataset-v1/final_cache"
TRAIN_CACHE = os.path.join(CACHE_DIR, "train_tokens.npy")
VAL_CACHE = os.path.join(CACHE_DIR, "val_tokens.npy")

if os.path.exists(TRAIN_CACHE) and os.path.exists(VAL_CACHE):
    train_tokens = np.load(TRAIN_CACHE)
    val_tokens = np.load(VAL_CACHE)
    print(f"‚úÖ Loaded: Train {train_tokens.shape}, Val {val_tokens.shape}")
else:
    print("‚ö†Ô∏è  Cache files not found, using dummy data for demo")
    # Create dummy data for demonstration
    train_tokens = np.random.randint(0, cfg.vocab_size, size=(1000, cfg.max_len))
    val_tokens = np.random.randint(0, cfg.vocab_size, size=(100, cfg.max_len))
    print(f"‚úÖ Created dummy: Train {train_tokens.shape}, Val {val_tokens.shape}")

def create_batches(tokens, batch_size, shuffle=False):
    n_samples = len(tokens)
    if shuffle:
        indices = np.random.permutation(n_samples)
        tokens = tokens[indices]
    n_batches = n_samples // batch_size
    tokens = tokens[:n_batches * batch_size]
    batches = tokens.reshape(n_batches, batch_size, -1)
    return batches

train_batches = create_batches(train_tokens, cfg.global_batch, shuffle=True)
val_batches = create_batches(val_tokens, cfg.global_batch, shuffle=False)

print(f"‚úÖ Created {len(train_batches)} train batches, {len(val_batches)} val batches")

# Load or create tokenizer (simplified for demo)
print("\nüì¶ Setting up tokenizer...")

# Use a simple approach for this demo
class SimpleTokenizer:
    def __init__(self, vocab_size):
        self.vocab_size = vocab_size
    
    def encode(self, text):
        # Simple character-level encoding for demo
        return type('obj', (object,), {'ids': [hash(c) % self.vocab_size for c in text.replace(' ', '_')]})()
    
    def decode(self, ids):
        return ''.join([str(i) for i in ids])
    
    def token_to_id(self, token):
        return hash(token) % self.vocab_size

tok = SimpleTokenizer(cfg.vocab_size)
print(f"‚úÖ Tokenizer ready with vocab_size: {cfg.vocab_size}")

# Initialize model with FastArray compression
print("\n" + "="*70)
print("üîÑ INITIALIZING MODEL WITH FASTARRAY COMPRESSION".center(70))
print("="*70)

sharding_rules, activation_sharding = create_sharding_rules(mesh, cfg)

with mesh:
    rng = random.PRNGKey(cfg.seed)
    rng, init_rng = random.split(rng)

    state, param_count = create_train_state_with_fastarray_compression(init_rng, cfg, mesh, sharding_rules)
    state = jax.device_put_replicated(state, jax.local_devices())

print(f"‚úÖ Model with FastArray compression ready!")

# Training loop setup (same as original)
print("\n" + "="*70)
print("üî• TRAINING WITH FASTARRAY COMPRESSION üî•".center(70))
print("="*70)

history = {
    'train_loss': [], 'train_ce': [], 'train_z': [],
    'train_acc': [], 'train_ppl': [],
    'val_loss': [], 'val_ce': [], 'val_z': [],
    'val_acc': [], 'val_ppl': []
}

steps_per_epoch = len(train_batches)
total_train_steps = steps_per_epoch * cfg.epochs
eta_tracker = ETATracker(total_train_steps)

global_step = 0
best_val_loss = float('inf')
patience = 3
no_improve_count = 0
check_interval = min(2000, len(train_batches))  # Reduced for demo

print(f"üìö Starting training for {cfg.epochs} epochs ({total_train_steps} steps total)")

# Main training loop
for epoch in range(cfg.epochs):
    print(f"\n{'‚îÄ'*70}")
    print(f"üìö Epoch {epoch + 1}/{cfg.epochs}")
    print(f"{'‚îÄ'*70}")

    epoch_metrics = []
    epoch_start = time.time()

    for step, batch in enumerate(train_batches):
        try:
            n_devices = len(jax.local_devices())
            batch_size = batch.shape[0]
            per_device = batch_size // n_devices

            if batch_size % n_devices != 0:
                continue

            # Reshape batch for pmap
            batch = batch[:n_devices * per_device].reshape(n_devices, per_device, -1)

            step_start = time.time()
            state, metrics = train_step(
                state, batch, cfg.use_z_loss, cfg.z_loss_weight,
                cfg.label_smoothing
            )

            jax.block_until_ready(metrics)
            step_time = time.time() - step_start

            global_step += 1
            eta_tracker.update(global_step, step_time)

            metrics = tree_util.tree_map(lambda x: x[0], metrics)
            epoch_metrics.append(metrics)

            if global_step % check_interval == 0:
                print(f"\n  üîç Quick validation check at step {global_step}...")
                quick_val_metrics = []
                for i, val_batch in enumerate(val_batches[:10]):  # Just first 10 for speed
                    if len(val_batch) >= n_devices * 4:  # Ensure batch is large enough
                        val_batch = val_batch[:n_devices * 4].reshape(n_devices, 4, val_batch.shape[-1])
                        val_m = eval_step(
                            state, val_batch, cfg.use_z_loss, cfg.z_loss_weight, cfg.label_smoothing
                        )
                        val_m = tree_util.tree_map(lambda x: x[0], val_m)
                        quick_val_metrics.append(val_m)

                if quick_val_metrics:  # Only if we have metrics
                    quick_val_loss = np.mean([m['ce_loss'] + m['z_loss'] for m in quick_val_metrics])
                    quick_val_acc = np.mean([m['accuracy'] for m in quick_val_metrics])

                    print(f"     Val loss: {quick_val_loss:.4f}, acc: {quick_val_acc*100:.2f}%")

                    if quick_val_loss < best_val_loss:
                        best_val_loss = quick_val_loss
                        no_improve_count = 0
                        print(f"     ‚ú® New best! Saving...")
                        # Save using compressed parameters
                        final_params = tree_util.tree_map(lambda x: x[0] if hasattr(x, '__getitem__') else x, state.compressed_params)
                        checkpoints.save_checkpoint(
                            ckpt_dir=CHECKPOINT_DIR,
                            target=final_params,  # This uses the compressed params
                            step=global_step,
                            prefix='best_',
                            overwrite=True,
                            keep=1
                        )
                    else:
                        no_improve_count += 1
                        print(f"     No improvement ({no_improve_count}/{patience})")

                        if no_improve_count >= patience:
                            print(f"\n‚ö†Ô∏è  Early stopping! No improvement for {patience} checks.")
                            break

                    quick_val_metrics.clear()

        except Exception as e:
            print(f"‚ùå Error at step {step}: {e}")
            import traceback
            traceback.print_exc()
            break

        if (step + 1) % 10 == 0 or (step + 1) == len(train_batches):
            recent_metrics = epoch_metrics[-10:]
            avg_ce = np.mean([m['ce_loss'] for m in recent_metrics])
            avg_z = np.mean([m['z_loss'] for m in recent_metrics])
            avg_acc = np.mean([m['accuracy'] for m in recent_metrics])
            avg_ppl = np.mean([m['perplexity'] for m in recent_metrics])
            total_loss = avg_ce + avg_z

            epoch_pct = ((step + 1) / len(train_batches)) * 100

            print(f"  Step {step+1:>4}/{len(train_batches)} ‚îÇ "
                  f"loss: {total_loss:.4f} ‚îÇ ppl: {avg_ppl:.2f} ‚îÇ "
                  f"acc: {avg_acc*100:.2f}% ‚îÇ {step_time:.2f}s")

            if (step + 1) % 50 == 0:
                print(f"     Epoch: {epoch_pct:.1f}% ‚îÇ Elapsed: {eta_tracker.get_elapsed()} ‚îÇ ETA: {eta_tracker.get_eta()}")

    if no_improve_count >= patience:
        break

    # Epoch summary
    if epoch_metrics:
        train_ce = np.mean([m['ce_loss'] for m in epoch_metrics])
        train_z = np.mean([m['z_loss'] for m in epoch_metrics])
        train_loss = train_ce + train_z
        train_acc = np.mean([m['accuracy'] for m in epoch_metrics])
        train_ppl = np.mean([m['perplexity'] for m in epoch_metrics])

        history['train_loss'].append(float(train_loss))
        history['train_ce'].append(float(train_ce))
        history['train_z'].append(float(train_z))
        history['train_acc'].append(float(train_acc))
        history['train_ppl'].append(float(train_ppl))

        epoch_metrics.clear()

        # Validation (simplified for demo)
        print(f"\n  üîç Running validation (simplified)...")
        val_metrics = []
        val_start = time.time()

        for i, batch in enumerate(val_batches[:5]):  # Just first 5 batches for demo speed
            if len(batch) >= n_devices * 4:
                batch = batch[:n_devices * 4].reshape(n_devices, 4, batch.shape[-1])
                metrics = eval_step(
                    state, batch, cfg.use_z_loss, cfg.z_loss_weight, cfg.label_smoothing
                )
                metrics = tree_util.tree_map(lambda x: x[0], metrics)
                val_metrics.append(metrics)

        val_time = time.time() - val_start
        if val_metrics:
            val_ce = np.mean([m['ce_loss'] for m in val_metrics])
            val_z = np.mean([m['z_loss'] for m in val_metrics])
            val_loss = val_ce + val_z
            val_acc = np.mean([m['accuracy'] for m in val_metrics])
            val_ppl = np.mean([m['perplexity'] for m in val_metrics])

            history['val_loss'].append(float(val_loss))
            history['val_ce'].append(float(val_ce))
            history['val_z'].append(float(val_z))
            history['val_acc'].append(float(val_acc))
            history['val_ppl'].append(float(val_ppl))

        val_metrics.clear()

        # üß™ TEST GENERATION - The money saver!
        print("\n" + "üß™ TESTING MODEL GENERATION (using FastArray model)".center(70))
        test_prompts = [
            f"{CHAT_USER_PREFIX} Hello, how are you?\n{CHAT_ASSISTANT_PREFIX}",
            f"{CHAT_USER_PREFIX} What is 2+2?\n{CHAT_ASSISTANT_PREFIX}",
        ]

        all_tests_passed = True
        for test_prompt in test_prompts:
            try:
                passed = test_generation(state, tok, cfg, global_step, test_prompt)
                if not passed:
                    all_tests_passed = False
            except Exception as e:
                print(f"‚ùå Generation failed: {e}")
                all_tests_passed = False

        # Quality gates
        if len(history['val_ppl']) > 0 and history['val_ppl'][-1] > 100 and epoch > 0:
            print(f"\n‚ö†Ô∏è  HIGH PERPLEXITY: {history['val_ppl'][-1]:.2f}")
            all_tests_passed = False

        if len(history['val_acc']) > 0 and history['val_acc'][-1] < 0.1 and epoch > 0:
            print(f"\n‚ö†Ô∏è  LOW ACCURACY: {history['val_acc'][-1]*100:.2f}%")
            all_tests_passed = False

        if not all_tests_passed:
            print("\n" + "="*70)
            print("‚ö†Ô∏è  QUALITY ISSUES DETECTED".center(70))
            print("="*70)

        # Garbage collection
        import gc
        gc.collect()
        jax.clear_caches()

        # Save best model using compressed parameters
        if len(history['val_loss']) > 0 and history['val_loss'][-1] < best_val_loss:
            best_val_loss = history['val_loss'][-1]
            print(f"\n  üíæ New best validation loss: {best_val_loss:.4f}")

            # Save using compressed parameters
            final_params = tree_util.tree_map(lambda x: x[0] if hasattr(x, '__getitem__') else x, state.compressed_params)
            checkpoints.save_checkpoint(
                ckpt_dir=CHECKPOINT_DIR,
                target=final_params,
                step=epoch + 1,
                prefix='best_',
                overwrite=True
            )

        epoch_time = time.time() - epoch_start

        print(f"{'‚îÄ'*70}")
        print(f"üìä Epoch {epoch + 1} Summary:")
        if 'train_loss' in locals() and 'val_loss' in locals():
            print(f"   Train: loss={train_loss:.4f}, ppl={train_ppl:.2f}, acc={train_acc*100:.2f}%")
            print(f"   Val:   loss={val_loss:.4f}, ppl={val_ppl:.2f}, acc={val_acc*100:.2f}%")
        print(f"   Time:  {epoch_time:.1f}s")
        print(f"   Best:  val_loss={best_val_loss:.4f}")
        print(f"{'‚îÄ'*70}")

print("\n" + "="*70)
print("üéâ TRAINING WITH FASTARRAY COMPRESSION COMPLETE! üéâ".center(70))
print("="*70)

# Final summary
print(f"\nüìä FINAL RESULTS WITH FASTARRAY INTEGRATION:")
if history['train_loss']:
    print(f"   Train: loss={history['train_loss'][-1]:.4f}")
if history['val_loss']:
    print(f"   Val:   loss={history['val_loss'][-1]:.4f}")
print(f"   Best:  val_loss={best_val_loss:.4f}")
print(f"   Total params: {param_count:,} (~{param_count/1e6:.1f}M)")

# Calculate total compression benefit
final_compression_stats = get_compression_stats(state.compressed_params)
print(f"\nüìà COMPRESSION BENEFITS:")
print(f"   Memory saved: {final_compression_stats['saved_mb']:.2f}MB")
print(f"   Compression ratio: {final_compression_stats['compression_ratio']:.1f}x")

print(f"\n‚úÖ FastArray integration successful!")
print(f"‚úÖ Model trained with compressed parameters")
print(f"‚úÖ Ready for TPU deployment with significant memory savings!")